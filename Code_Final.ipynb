{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92908f92-e4e6-404c-86ed-817b6c400ccd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, BucketedRandomProjectionLSH, VectorSlicer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import array, create_map, struct, monotonically_increasing_id, row_number\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import random\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "from pyspark.sql.functions import count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41077528-1a75-4ab7-a458-e087294baac0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df745745-7e7f-4b97-a8d0-b479cb091dc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Preprocessing and SMOTE helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a00c655-949d-469a-8bcc-5b81e5c58535",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Preprocess function to convert dataframe columns to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0f28af5-ab54-48eb-90cb-a52772c3d976",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(spark_df,sel_fetures, target_column):\n",
    "    assembler = VectorAssembler(inputCols = sel_fetures, outputCol = 'features')\n",
    "    processedDF = assembler.transform(spark_df).withColumnRenamed(target_column,'label').select(\"features\",\"label\")\n",
    "    return processedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16d01de9-54d6-456c-b95b-b246bfa50351",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Function to get the label counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1931f0b5-da9c-487e-93d4-b934497d757f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getLabelCounts(dfvectorized):\n",
    "    label_counts = dfvectorized.groupBy(\"label\").count().collect()\n",
    "    label_counts_dict = {}\n",
    "    maj_count = 0\n",
    "\n",
    "    for r in label_counts:\n",
    "        label_counts_dict[r[\"label\"]] = r[\"count\"]\n",
    "        maj_count = max(maj_count,r[\"count\"])\n",
    "        \n",
    "    return maj_count, label_counts_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ebfebd-3b47-42e5-b64d-4330e0f0216c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SMOTE function to implement smote in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716b488a-2513-4c73-9af8-74454aadc57b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def SMOTE(dfvectorized, label_counts_dict, maj_count):\n",
    "    res = []\n",
    "\n",
    "    for label, instances in label_counts_dict.items():\n",
    "\n",
    "        if maj_count-instances == 0: continue\n",
    "\n",
    "        dataInput_min = dfvectorized[dfvectorized['label'] == label]\n",
    "\n",
    "        # LSH, bucketed random projection\n",
    "        brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", seed=41, \n",
    "                                          bucketLength=100)\n",
    "        # smote only applies on existing minority instances    \n",
    "        model = brp.fit(dataInput_min)\n",
    "        model.transform(dataInput_min)\n",
    "\n",
    "        # here distance is calculated from brp's param inputCol\n",
    "        self_join_w_distance = model.approxSimilarityJoin(dataInput_min, dataInput_min, float(\"inf\"), distCol=\"EuclideanDistance\")\n",
    "\n",
    "        # remove self-comparison (distance 0)\n",
    "        self_join_w_distance = self_join_w_distance.filter(self_join_w_distance.EuclideanDistance > 0)\n",
    "\n",
    "        over_original_rows = Window.partitionBy(\"datasetA\").orderBy(\"EuclideanDistance\")\n",
    "\n",
    "        self_similarity_df = self_join_w_distance.withColumn(\"r_num\", F.row_number().over(over_original_rows))\n",
    "\n",
    "        self_similarity_df_selected = self_similarity_df.filter(self_similarity_df.r_num <= 5)\n",
    "\n",
    "        over_original_rows_no_order = Window.partitionBy('datasetA')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # two udf for vector add and subtract, subtraction include a random factor [0,1]\n",
    "        subtract_vector_udf = F.udf(lambda arr: random.uniform(0, 1)*(arr[0]-arr[1]), VectorUDT())\n",
    "        add_vector_udf = F.udf(lambda arr: arr[0]+arr[1], VectorUDT())\n",
    "\n",
    "        # retain original columns\n",
    "        original_cols = dataInput_min.columns\n",
    "\n",
    "\n",
    "\n",
    "        df_random_sel = self_similarity_df_selected.withColumn(\"rand\", F.rand()).withColumn('max_rand', F.max('rand').over(over_original_rows_no_order))\\\n",
    "                                .where(F.col('rand') == F.col('max_rand')).drop(*['max_rand','rand','r_num'])\n",
    "\n",
    "        ### repeatign data\n",
    "        df_random_sel = df_random_sel.withColumn(\"row_id\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "        toAdd = maj_count - instances ## majority class count minus current class count\n",
    "        print(f\"Current class:{label}, instances:{instances}, toAdd:{toAdd}\")\n",
    "        rand_rownum = list(np.random.choice(a=instances,size=toAdd,replace=True))\n",
    "        rand_rownum = [int(r) for r in rand_rownum]\n",
    "        ## Creatign a data frame with number of required rows\n",
    "        df_n = spark.createDataFrame(zip(rand_rownum),[\"row_no\"])\n",
    "\n",
    "        ##mJoining both\n",
    "        df_random_sel_n = df_n.join(df_random_sel, df_n.row_no == df_random_sel.row_id, \"inner\")\n",
    "\n",
    "        # create synthetic feature numerical part\n",
    "        df_vec_diff = df_random_sel_n.select('*', subtract_vector_udf(F.array('datasetA.features', 'datasetB.features')).alias('vec_diff'))\n",
    "        df_vec_modified = df_vec_diff.select('*', add_vector_udf(F.array('datasetA.features', 'vec_diff')).alias('features')).withColumn('label',F.lit(label))\n",
    "        \n",
    "        df_vec_modified = df_vec_modified.drop(*['row_no','row_id','datasetA','datasetB','vec_diff','EuclideanDistance'])\n",
    "\n",
    "        res.append(df_vec_modified)\n",
    "\n",
    "\n",
    "\n",
    "    dfunion = reduce(DataFrame.unionAll, res)\n",
    "\n",
    "    # union synthetic instances with original full (both minority and majority) df\n",
    "    oversampled_df = dfunion.union(dfvectorized.select(dfunion.columns))\n",
    "    return oversampled_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc326cfa-677d-4afb-9f14-010297ad87b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Main function to implement SMOTE which uses the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fdc0537-da1d-4b6d-8a30-bd67ae2e7d15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def genSyntheticData(spark_df, sel_features, target_column):\n",
    "    print(\"Sending the data to preprocess step\")\n",
    "    dfvectorized = preprocess(spark_df,sel_features, target_column)\n",
    "    print(\"Getting required label statistics\")\n",
    "    maj_count, label_counts_dict = getLabelCounts(dfvectorized)\n",
    "    print(\"Calling the SMOTE method\")\n",
    "    oversampled_df = SMOTE(dfvectorized, label_counts_dict, maj_count)\n",
    "    return oversampled_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adde7d7c-8e5b-4bb4-bd75-c0b26b196d24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a6f50d-1126-41a2-ab53-cf3f61bc7ac1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b77b2e-97f4-491e-9298-46bc54856783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filepath = \"/mnt/team16/covtype_final.csv\"\n",
    "df = spark.read\\\n",
    "    .format('csv')\\\n",
    "    .option('header', True)\\\n",
    "    .load(filepath)\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ebc964-1d8e-478b-9433-75fe94626b2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### selecting the final columns for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1328816b-b834-4e2a-8e28-ce651a8bcd63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selectedcols = ['Elevation',\n",
    " 'Aspect',\n",
    " 'Slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3a65fcb-af27-4a83-adf2-b2e5aa5011ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.select( selectedcols + ['Cover_Type',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b639be8b-9cca-48c6-b1b9-a222b1bdcd8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Converting the string values to integertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02dae735-b4b8-48ba-87cc-9382f7175091",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.select([F.col(c).cast(\"integer\") for c in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a190cc-32ea-4520-bf10-4953107c62a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Normalise function which uses min-max normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc16f06-4ddb-433d-ba21-8d50a5ea5984",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalisation(df, inputCols):\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    \n",
    "    inputCols = inputCols\n",
    "    outputCol = 'features'\n",
    "    output = 'Cover_Type'\n",
    "    #inputCols.remove(output)\n",
    "    assemblers = [VectorAssembler(inputCols = [col],\n",
    "                                  outputCol = col+'_vec',\n",
    "                                  handleInvalid=\"keep\") for col in inputCols]\n",
    "\n",
    "    featureScaler = [MinMaxScaler(inputCol = col + '_vec', \n",
    "                                  outputCol = col + '_scaled', \n",
    "                                  min = 0.0, \n",
    "                                  max = 1.0) for col in inputCols]\n",
    "\n",
    "    pipeline_scaler = Pipeline(stages = assemblers + featureScaler)\n",
    "    scaled_fit_df = pipeline_scaler.fit(df)\n",
    "    scaled_df = scaled_fit_df.transform(df)\n",
    "\n",
    "    scaled_df.show(5)\n",
    "    \n",
    "    return(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f729a6fe-1125-48f8-873e-18ed8eb692d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Function for Oversampling using Random Oversampling Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "399afb6c-00a6-4552-b58f-27d7b21e50db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def oversample(df_oversample):\n",
    "    from pyspark.sql.functions import count, col\n",
    "    distinct_values = df_oversample.groupBy(\"Cover_Type\")\\\n",
    "        .agg(count(\"*\").alias(\"count\"))\\\n",
    "        .orderBy(col(\"Cover_Type\").asc())\n",
    "    distinct_values.show()\n",
    "\n",
    "    majority = distinct_values.orderBy(col('count').desc()).limit(1).collect()\n",
    "    print(majority)\n",
    "\n",
    "    majority_class = df_oversample.filter(col(\"Cover_Type\") == 2)\n",
    "\n",
    "    inputCols = df_oversample.columns\n",
    "    inputCols.remove('Cover_Type')\n",
    "\n",
    "    classes = [i for i in range(1,8) if i!= majority[0][0]]\n",
    "    print(classes)\n",
    "\n",
    "    majority_count = df_oversample.filter(df_oversample['Cover_Type'] == majority[0][0]).count()\n",
    "\n",
    "    minority_upsampled = []\n",
    "    for i in classes:\n",
    "        minority_class = df_oversample.filter(col(\"Cover_Type\") == i)\n",
    "\n",
    "        minority_upsampled.append(minority_class.sample(True, majority_count / minority_class.count(), seed=20))\n",
    "\n",
    "    print(\"Upsampled Minority\", len(minority_upsampled))\n",
    "\n",
    "    upsampled_data = majority_class\n",
    "    for upsample in minority_upsampled:\n",
    "        upsampled_data = upsampled_data.union(upsample)\n",
    "\n",
    "    upsampled_data.groupBy(\"Cover_Type\").count().show()\n",
    "\n",
    "    print(upsampled_data.show(10)) \n",
    "\n",
    "    return(upsampled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdcd83a2-c196-4eec-b5d2-8d3a6d2e21ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Function for Undersampling using Random Undersampling Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211aec07-22f2-41d8-bd7a-5cba84398604",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def undersample(df_undersample):\n",
    "    from pyspark.sql.functions import count, col \n",
    "    distinct_values = df_undersample.groupBy(\"Cover_Type\")\\\n",
    "        .agg(count(\"*\").alias(\"count\"))\\\n",
    "        .orderBy(col(\"Cover_Type\").asc())\n",
    "    distinct_values.show()\n",
    "\n",
    "    minority = distinct_values.orderBy(col('count').asc()).limit(1).collect()\n",
    "    print(minority)\n",
    "\n",
    "    minority_class = df_undersample.filter(col(\"Cover_Type\") == 4)\n",
    "\n",
    "    inputCols = df_undersample.columns\n",
    "    inputCols.remove('Cover_Type')\n",
    "\n",
    "    classes = [i for i in range(1, 8) if i != minority[0][0]]\n",
    "    print(classes)\n",
    "\n",
    "    minority_count = df_undersample.filter(df_undersample['Cover_Type'] == minority[0][0]).count()\n",
    "\n",
    "    majority_undersampled = []\n",
    "    for i in classes:\n",
    "        majority_class = df_undersample.filter(col(\"Cover_Type\") == i)\n",
    "\n",
    "        majority_undersampled.append(majority_class.sample(True, minority_count / majority_class.count(), seed=20))\n",
    "\n",
    "    print(\"Undersampled Majority\", len(majority_undersampled))\n",
    "\n",
    "    undersampled_data = minority_class\n",
    "    for undersample in majority_undersampled:\n",
    "        undersampled_data = undersampled_data.union(undersample)\n",
    "\n",
    "    undersampled_data.groupBy(\"Cover_Type\").count().show()\n",
    "    print(undersampled_data.show(10))\n",
    "    return (undersampled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86291599-ea30-428b-81e7-22cc98be36e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Function for converting the dataset column values in form of vector with the column names as features, label\n",
    "### where features is the features and label is the output column (Cover_Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50875356-780f-4704-a306-9f497b4adfb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def processed_vector(df):\n",
    "  from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "  sel_fetures = [\"Elevation\",\"Aspect\",\"Slope\"]\n",
    "  target_column = \"Cover_Type\"\n",
    "  assembler = VectorAssembler(inputCols = sel_fetures, outputCol = 'features')\n",
    "  processedDF = assembler.transform(df).withColumnRenamed(target_column,'label').select(\"features\",\"label\")\n",
    "  return processedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594c42cf-866e-4f62-a942-f3a3b26fbd5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Normalising the dataset using the normalise helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a6e0e76-687e-40fe-963a-aea81dd5784c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inputCols = [\"Elevation\", \"Aspect\", \"Slope\"]\n",
    "df = normalisation(df, inputCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0483a04f-c546-4475-85d8-30a2269f90ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Removing the intermediate vector columns created during normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed2b3ad-2cd8-4f91-80e3-646b12e151e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col in inputCols or \"_vec\" in col:\n",
    "        df = df.drop(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3809a105-5060-4355-b9ff-e73df9fbf915",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Converting the vectorised scaled columns to Doubletype column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85cfc049-75e3-4b5b-8f80-4b935a2055ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "udf1 = F.udf(lambda x: float(x[0]), DoubleType())\n",
    "for col in df.columns:\n",
    "    if \"_scaled\" in col:\n",
    "        df = df.withColumn(col,udf1(col).alias(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3acdd3b2-bbae-4616-807b-fe0b6b004096",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Renaming the scaled columns to the original names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34fd471-3b16-4eb5-972c-ba841ec54561",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('Elevation_scaled','Elevation')\n",
    "df = df.withColumnRenamed('Slope_scaled','Slope')\n",
    "df = df.withColumnRenamed('Aspect_scaled','Aspect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b71bb1-d36a-4fcc-b4f5-2eeec0166e1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Splitting the dataset in test and train using 90-10 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c3521a6-2ffe-4d64-b8d0-e185748a65c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainDF, testDF = df.randomSplit([.9,.1], seed=41)\n",
    "print(f\"Training data examples: {trainDF.count()} \\nTesting data examples: {testDF.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c7dfc2c-dcaf-4e88-935d-19ec7f5a3229",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af1f93c-bc6a-4861-890a-66b2f9870960",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Resampling the train dataframe to hold 15% of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6751c8f4-1b30-47f5-8319-7df4d8ef4378",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainDFs = trainDF.sample(fraction = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1515ee2-90c2-4655-9fa5-e8dfb76719ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainDFs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c434c6-3a6f-49ae-a5d0-8d4a2e24b246",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Function for model train and cross-validation which returns F1 score and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03be8b32-ec3d-4c5b-a1b5-af420dfe08b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def model_train_validation(train, test):\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "    from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    from pyspark.ml.feature import MinMaxScaler\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    from pyspark.sql.functions import count, col\n",
    "\n",
    "\n",
    "    VectorAssembler1 = VectorAssembler(inputCols=train.columns,\n",
    "                                       outputCol='features')\n",
    "\n",
    "    evaluator1 = MulticlassClassificationEvaluator(metricName='f1',\n",
    "                                                  predictionCol='prediction',\n",
    "                                                  labelCol='label')\n",
    "\n",
    "    evaluator2 = MulticlassClassificationEvaluator(metricName='accuracy',\n",
    "                                                  predictionCol='prediction',\n",
    "                                                  labelCol='label')\n",
    "\n",
    "    lr = LogisticRegression(labelCol='label')\n",
    "\n",
    "    paramGrid = ParamGridBuilder()\\\n",
    "        .addGrid(lr.maxIter, [10])\\\n",
    "        .build()\n",
    "\n",
    "    cv1 = CrossValidator(estimator=lr,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=evaluator1)\n",
    "    \n",
    "    cv2 = CrossValidator(estimator=lr,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=evaluator2)\n",
    "\n",
    "    pipeline1 = Pipeline(stages=[cv1])\n",
    "    pipeline2 = Pipeline(stages=[cv2])\n",
    "    \n",
    "    \n",
    "    pipelineModel1 = pipeline1.fit(train)\n",
    "    pipelineModel2 = pipeline2.fit(train)\n",
    "\n",
    "    predictions1 = pipelineModel1.transform(test)\n",
    "    predictions2 = pipelineModel2.transform(test)\n",
    "    \n",
    "    f1 = evaluator1.evaluate(predictions1)\n",
    "    accuracy = evaluator2.evaluate(predictions2)\n",
    "\n",
    "    print(\"F1:\", f1)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    return f1,accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4d2f0b-7dd8-4486-a415-e5dcc892765d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Converting the test dataframe as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc618c0-a4da-43aa-99b7-501308079ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "testDF = processed_vector(testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58f38397-3556-41cb-8f66-02f858dc8076",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Building Base Model using train dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a59bda51-d96a-4837-a102-bdd2cca8d57b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Converting train dataframe as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd3d28b-7bfb-4bcc-bfc5-107a2186ea59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baseDF = processed_vector(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6735a7e6-4242-4344-a50c-ad4bef30b060",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Building Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5beab726-141d-4ab8-a782-c0383b04023f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f1,accuracy = model_train_validation(baseDF, testDF)\n",
    "print(\"F1 of Base Model: \", f1)\n",
    "print(\"Accuracy of Base Model: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a4c13b-d274-4ddb-8840-17b9e82db5da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Building Oversampled Dataset and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aaf90a7-c3f3-420f-a334-dac8caa96f64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Oversampling train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48de161-4e7e-45b8-a6bb-6a46903de877",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "oversampled_df = oversample(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db3f3c3-5b2d-460d-ae7c-7c61cafbdc61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Converting oversampled dataframe as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f709c0b-b6c3-4c7d-83b8-beb584aea6ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "oversampled_df = processed_vector(oversampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750f7ba5-dba8-46ec-b045-1f981b822b6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Building Oversampled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27acba0-5ca6-40ce-8d93-3d0ec4c8d33a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f1,accuracy = model_train_validation(oversampled_df, testDF)\n",
    "print(\"F1 of Oversampled Model: \", f1)\n",
    "print(\"Accuracy of Oversampled Model: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a15369-84bf-4a49-9cf9-50fe14a67e7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Building Undersampled Dataset and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada92a06-986e-453b-bb31-b564308a0f4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Undersampling train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b28bb62-0e55-497a-be85-434846f6bb8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "undersampled_df = undersample(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6952674-01ac-4f89-88f4-5ac5542ffe7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Converting undersampled dataframe as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d23f87a-c9ad-46f6-bc4d-852ebc451852",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "undersampled_df = processed_vector(undersampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5af75e5-7df1-45a6-90f5-34c26e183de3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Building Undersampled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58788d09-ec15-4b55-83ff-eb458aa3c057",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f1,accuracy = model_train_validation(undersampled_df, testDF)\n",
    "print(\"F1 of Undersampled Model: \", f1)\n",
    "print(\"Accuracy of Undersampled Model: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c753fbc-4231-418d-a22b-86bdce01806d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Applying SMOTE Algorithm on the train dataframe using SMOTE main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee9c7ed-d3e1-4c06-8173-c9265650da0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "oversampled_smote_df = genSyntheticData(trainDFs, [\"Elevation\",\"Aspect\",\"Slope\"], \"Cover_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257495e3-42cb-445e-8472-4bc9b1613cca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "oversampled_smote_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f790ea5e-8721-419f-9311-6c12adee0fc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "oversampled_smote_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab86105-5455-459a-a33c-25c9a0c8b526",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Building SMOTE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e42329-daff-4410-b4cd-701fa23babd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f1,accuracy = model_train_validation(oversampled_smote_df, testDF)\n",
    "print(\"F1 of SMOTE Model: \", f1)\n",
    "print(\"Accuracy of SMOTE Model: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54ba145e-1665-4e1e-a0b9-096071a9a769",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b051c095-3874-41be-b687-655f1302a9bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435dddb0-321d-482c-bd9f-bb0cbba6bb8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Plotting Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bdb8788-0ae9-4214-bd9d-a50c03379fe0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a930558-c1ea-4ea8-ba08-9f34553ed226",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_oversampled_df = oversampled_df.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='Elevation', y='Aspect',data=pd_oversampled_df, hue='Cover_Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7475de4-e8cf-4dbe-a30e-0098ed57d6ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_undersampled_df = undersampled_df.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='Elevation', y='Aspect',data=pd_undersampled_df, hue='Cover_Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7bece5f-307f-48ec-a9fc-bf7775aabc2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_smotesampled_df = oversampled_smote_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38aaa85c-c475-45a7-97fd-2779b61d5b08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_smotesampled_df['Elevation'] = pd_smotesampled_df[\"features\"].apply(lambda X:X[0])\n",
    "pd_smotesampled_df['Aspect'] = pd_smotesampled_df[\"features\"].apply(lambda X:X[1])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='Elevation', y='Aspect',data=pd_smotesampled_df, hue='label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66eaed55-769a-408c-8c59-437054dd7949",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_base_df = trainDF.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='Elevation', y='Aspect',data=pd_base_df, hue='Cover_Type')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Code_Final",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
